{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhgoMNEp0ImO"
      },
      "source": [
        "Mounting drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GARQ7OT5z3ta"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgRREE9z0pT3"
      },
      "source": [
        "Installing lime (its not pre-instaled on colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJ4xSXjc0Ami"
      },
      "outputs": [],
      "source": [
        "!pip install lime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QOQb-oddPIq"
      },
      "source": [
        "Installing other dependencies (Its not pre-installed if you are using TPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOBUaJL_0tIn"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mru13-rVz-dP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tf_keras.models import Sequential, Model, load_model\n",
        "from tf_keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout, GlobalAveragePooling2D, Input, SpatialDropout2D, GaussianNoise\n",
        "from tf_keras.regularizers import l2\n",
        "from tf_keras.metrics import Precision, Recall, AUC\n",
        "from tf_keras import layers, models\n",
        "from tf_keras.utils import array_to_img\n",
        "from tf_keras.preprocessing import image\n",
        "from tf_keras.callbacks import EarlyStopping\n",
        "from tf_keras.optimizers import Adam, AdamW\n",
        "from tf_keras.applications import EfficientNetB0,Xception\n",
        "from tf_keras import regularizers\n",
        "from tf_keras.mixed_precision import set_global_policy\n",
        "from tf_keras.callbacks import (\n",
        "    ReduceLROnPlateau,\n",
        "    ModelCheckpoint,\n",
        "    TerminateOnNaN\n",
        ")\n",
        "from tf_keras.activations import swish, tanh\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from collections import Counter\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy8cVXHV0x64"
      },
      "source": [
        "Small functions that will be used all around the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwvr3b-30HsS"
      },
      "outputs": [],
      "source": [
        "#Count total images in a dataset\n",
        "def count_images(dataset):\n",
        "    total = 0\n",
        "    for images, _ in dataset:\n",
        "        total += images.shape[0]\n",
        "    return total\n",
        "\n",
        "# Função para rescale\n",
        "def rescale_image(image):\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image\n",
        "\n",
        "\n",
        "#Prepare the dataset\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "def prepare(ds, shuffle=False):\n",
        "\n",
        "    ds = ds.map(lambda x, y: (rescale_image(x), y), num_parallel_calls=AUTOTUNE)\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(1000)\n",
        "\n",
        "    return ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "#Prepare a single image\n",
        "def prepare_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, (64, 64))\n",
        "    img = img / 255.0\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "def display_single_image(img_path, img_size, print_shape = False):\n",
        "  '''Displays an image, and depending on need, can print the input shape'''\n",
        "\n",
        "  # Load Image\n",
        "  img = image.load_img(img_path, target_size= img_size)\n",
        "  img_tensor = image.img_to_array(img)\n",
        "  img_tensor = np.expand_dims(img_tensor, axis = 0)\n",
        "\n",
        "  # Light preprocessing\n",
        "  img_tensor /= 255.\n",
        "\n",
        "  # Print the shape\n",
        "  if print_shape:\n",
        "    print(img_tensor.shape)\n",
        "\n",
        "  # Display the image\n",
        "  plt.imshow(img_tensor[0])\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "def plot_graphs(history):\n",
        "  # Gráfico de Loss\n",
        "  plt.plot(history.history['loss'], label='Train Loss')\n",
        "  plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "  plt.xlabel('Épocas')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # Gráfico de Acurácia\n",
        "  plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "  plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "  plt.xlabel('Épocas')\n",
        "  plt.ylabel('Acurácia')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "#Show the classification report for the model\n",
        "def model_evaluation(model, test_ds, class_names=[\"AI-Generated Images\", \"Real Images\"]):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    batch_count = 0\n",
        "\n",
        "    for images, labels in test_ds:\n",
        "        batch_count += 1\n",
        "        print(f\"Processando batch {batch_count}\")\n",
        "\n",
        "        y_true.extend(labels.numpy())\n",
        "        predictions = model.predict(images, verbose=0)\n",
        "        y_pred.extend((predictions > 0.5).astype(int).flatten())\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "    # Matriz de Confusão\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names,\n",
        "                yticklabels=class_names)\n",
        "    plt.xlabel('Predito')\n",
        "    plt.ylabel('Real')\n",
        "    plt.title('Matriz de Confusão')\n",
        "    plt.show()\n",
        "\n",
        "#Test function for a single image\n",
        "def test_single_image(image_path, model):\n",
        "\n",
        "  processed_img = rescale_image(image_path)\n",
        "  prediction = model.predict(processed_img)\n",
        "\n",
        "  if prediction[0][0] > 0.5:\n",
        "    print(\"Imagem real. (Probabilidade: {:.2f}%)\".format(prediction[0][0] * 100))\n",
        "  else:\n",
        "    print(\"Deepfake detectado! (Probabilidade: {:.2f}%)\".format((1 - prediction[0][0]) * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Xy7pMC1CFC"
      },
      "source": [
        "Counting all the images on the dataset and display them organized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6cs4wAl1BYZ"
      },
      "outputs": [],
      "source": [
        "base_path = '/content/drive/MyDrive/TCC-Dataset/split'\n",
        "folders = {\n",
        "    'train': os.path.join(base_path, 'train'),\n",
        "    'test': os.path.join(base_path, 'test'),\n",
        "    'val': os.path.join(base_path, 'val')\n",
        "}\n",
        "\n",
        "extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.webp')\n",
        "\n",
        "counts = {\n",
        "    'real': {'train': 0, 'test': 0, 'val': 0, 'total': 0},\n",
        "    'fake': {'train': 0, 'test': 0, 'val': 0, 'total': 0}\n",
        "}\n",
        "\n",
        "for split in ['train', 'test', 'val']:\n",
        "    real_path = os.path.join(folders[split], 'real')\n",
        "    fake_path = os.path.join(folders[split], 'fake')\n",
        "\n",
        "    if os.path.exists(real_path):\n",
        "        counts['real'][split] = sum(\n",
        "            1 for file in os.listdir(real_path)\n",
        "            if file.lower().endswith(extensions)\n",
        "        )\n",
        "        counts['real']['total'] += counts['real'][split]\n",
        "\n",
        "    if os.path.exists(fake_path):\n",
        "        counts['fake'][split] = sum(\n",
        "            1 for file in os.listdir(fake_path)\n",
        "            if file.lower().endswith(extensions)\n",
        "        )\n",
        "        counts['fake']['total'] += counts['fake'][split]\n",
        "\n",
        "print(\"=== Contagem de Imagens ===\")\n",
        "print(f\"TOTAL REAIS: {counts['real']['total']}\")\n",
        "print(f\"TOTAL FAKES: {counts['fake']['total']}\\n\")\n",
        "\n",
        "print(\"Detalhes por pasta:\")\n",
        "for split in ['train', 'test', 'val']:\n",
        "    print(f\"\\n[{split.upper()}]\")\n",
        "    print(f\"Reais: {counts['real'][split]}\")\n",
        "    print(f\"Fakes: {counts['fake'][split]}\")\n",
        "    print(f\"Total: {counts['real'][split] + counts['fake'][split]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCMvRyyraJTu"
      },
      "source": [
        "Changing dataset size (Moving images to another directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-d_KJW3VaIrH"
      },
      "outputs": [],
      "source": [
        "# Caminhos principais\n",
        "base_path = '/content/drive/MyDrive/TCC-Dataset/split'\n",
        "backup_path = '/content/drive/MyDrive/TCC-Dataset/backup'\n",
        "\n",
        "# Certificar que a pasta de backup existe\n",
        "for split in ['train', 'val', 'test']:\n",
        "    for label in ['real', 'fake']:\n",
        "        os.makedirs(os.path.join(backup_path, split, label), exist_ok=True)\n",
        "\n",
        "# Estrutura de pastas reais e fakes\n",
        "paths = {\n",
        "    'real': {\n",
        "        'train': os.path.join(base_path, 'train', 'real'),\n",
        "        'val': os.path.join(base_path, 'val', 'real'),\n",
        "        'test': os.path.join(base_path, 'test', 'real')\n",
        "    },\n",
        "    'fake': {\n",
        "        'train': os.path.join(base_path, 'train', 'fake'),\n",
        "        'val': os.path.join(base_path, 'val', 'fake'),\n",
        "        'test': os.path.join(base_path, 'test', 'fake')\n",
        "    }\n",
        "}\n",
        "\n",
        "# Quantidade a mover por categoria e por pasta\n",
        "move_counts = {\n",
        "    'train': 7000,\n",
        "    'val': 1500,\n",
        "    'test': 1500\n",
        "}\n",
        "\n",
        "# Extensões válidas\n",
        "valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.webp')\n",
        "\n",
        "# Total movidos por categoria\n",
        "total_moved = {'real': 0, 'fake': 0}\n",
        "\n",
        "def mover_imagens(tipo):\n",
        "    for split, origem in paths[tipo].items():\n",
        "        destino = os.path.join(backup_path, split, tipo)\n",
        "\n",
        "        if not os.path.exists(origem):\n",
        "            print(f\"[ERRO] Pasta não encontrada: {origem}\")\n",
        "            continue\n",
        "\n",
        "        # Listar e embaralhar imagens\n",
        "        imagens = [f for f in os.listdir(origem) if f.lower().endswith(valid_extensions)]\n",
        "        random.shuffle(imagens)\n",
        "\n",
        "        mover_qtd = min(move_counts[split], len(imagens))\n",
        "        print(f\"[{split.upper()}][{tipo.upper()}] Movendo {mover_qtd} de {len(imagens)} imagens...\")\n",
        "\n",
        "        for img in imagens[:mover_qtd]:\n",
        "            try:\n",
        "                shutil.move(os.path.join(origem, img), os.path.join(destino, img))\n",
        "                total_moved[tipo] += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao mover {img}: {e}\")\n",
        "\n",
        "# Mover imagens reais e fakes\n",
        "mover_imagens('real')\n",
        "mover_imagens('fake')\n",
        "\n",
        "# Resultado final\n",
        "print(f\"\\n Total de imagens movidas:\")\n",
        "print(f\"Reais: {total_moved['real']}\")\n",
        "print(f\"Fakes: {total_moved['fake']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuvTGARcaj-d"
      },
      "source": [
        "Taking images from one directory and organize them on the dataset directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gx6hQ2onajL-"
      },
      "outputs": [],
      "source": [
        "# Caminho da pasta base\n",
        "wiki_base_path = '/content/drive/MyDrive/TCC-Dataset/wiki'\n",
        "output_base_path = '/content/drive/MyDrive/TCC-Dataset/split'\n",
        "\n",
        "# Destinos finais\n",
        "destinations = {\n",
        "    'train': os.path.join(output_base_path, 'train', 'real'),\n",
        "    'val': os.path.join(output_base_path, 'val', 'real'),\n",
        "    'test': os.path.join(output_base_path, 'test', 'real')\n",
        "}\n",
        "\n",
        "# Criar pastas de destino se não existirem\n",
        "for path in destinations.values():\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# Extensões válidas\n",
        "valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.webp')\n",
        "\n",
        "# Coletar todas as imagens das subpastas\n",
        "all_images = []\n",
        "for subfolder in sorted(os.listdir(wiki_base_path)):\n",
        "    subfolder_path = os.path.join(wiki_base_path, subfolder)\n",
        "    if os.path.isdir(subfolder_path):\n",
        "        for file in os.listdir(subfolder_path):\n",
        "            if file.lower().endswith(valid_extensions):\n",
        "                full_path = os.path.join(subfolder_path, file)\n",
        "                all_images.append(full_path)\n",
        "\n",
        "# Embaralhar as imagens\n",
        "random.shuffle(all_images)\n",
        "\n",
        "# Separar\n",
        "total = len(all_images)\n",
        "train_split = int(0.7 * total)\n",
        "val_split = int(0.85 * total)\n",
        "\n",
        "train_images = all_images[:train_split]\n",
        "val_images = all_images[train_split:val_split]\n",
        "test_images = all_images[val_split:]\n",
        "\n",
        "# Função para copiar imagens\n",
        "def copy_images(image_list, destination):\n",
        "    for img_path in image_list:\n",
        "        filename = os.path.basename(img_path)\n",
        "        dest_path = os.path.join(destination, filename)\n",
        "        shutil.copy2(img_path, dest_path)  # Use shutil.move(...) para mover em vez de copiar\n",
        "\n",
        "# Copiar imagens\n",
        "copy_images(train_images, destinations['train'])\n",
        "copy_images(val_images, destinations['val'])\n",
        "copy_images(test_images, destinations['test'])\n",
        "\n",
        "# Exibir resumo\n",
        "print(f\"Total de imagens encontradas: {total}\")\n",
        "print(f\"Treino: {len(train_images)}\")\n",
        "print(f\"Validação: {len(val_images)}\")\n",
        "print(f\"Teste: {len(test_images)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rl8wkCb1Psh"
      },
      "source": [
        "Loading the dataset and dividing them into train/val/test (0.7/0.15/0.15) + Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4NpuT_L1PLe"
      },
      "outputs": [],
      "source": [
        "# Caminhos\n",
        "base_path = '/content/drive/MyDrive/TCC-Dataset/split'\n",
        "\n",
        "# Parâmetros\n",
        "batch_size = 32\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Normalização\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "\n",
        "# Dataset de treino\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    f'{base_path}/train',\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    label_mode='binary'\n",
        ")\n",
        "\n",
        "# Dataset de validação\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    f'{base_path}/val',\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    label_mode='binary'\n",
        ")\n",
        "\n",
        "# Dataset de teste\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    f'{base_path}/test',\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    label_mode='binary'\n",
        ")\n",
        "\n",
        "# Aplicar normalização\n",
        "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "# Otimização do pipeline\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "le0LlcJQj0Bf"
      },
      "outputs": [],
      "source": [
        "# Caminhos\n",
        "base_path = '/content/drive/MyDrive/TCC-Dataset/split'\n",
        "\n",
        "# Parâmetros\n",
        "batch_size = 32\n",
        "img_height = 299 #Necessario mudar dependendo do modelo\n",
        "img_width = 299 #Necessario mudar dependendo do modelo\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Normalização\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "\n",
        "# Dataset de treino\n",
        "test2_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    base_path,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    label_mode='binary'  # ou 'categorical' para multi-classes\n",
        ")\n",
        "test2_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "test2_ds = test_ds.prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0HRczBy_kS2f"
      },
      "outputs": [],
      "source": [
        "model_evaluation(model, test2_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs8QkmXC1hIe"
      },
      "source": [
        "Printing the class names to see if its all correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mL2rwFz61gwt"
      },
      "outputs": [],
      "source": [
        "print(test_ds.class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avr64Bbt1r9z"
      },
      "source": [
        "Printing the dataset to see the shape and scale of images, and other useful information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLObuw331rlf"
      },
      "outputs": [],
      "source": [
        "print(train_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRFbkwHz1z5X"
      },
      "source": [
        "Plotting a single image to see what happens after the rescale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwvADnIq1zom"
      },
      "outputs": [],
      "source": [
        "img_path = '/content/drive/MyDrive/TCC-Dataset/Human Faces Dataset/AI-Generated Images/000001.jpg'\n",
        "img_size = (160, 160)\n",
        "img_size2 = (64, 64)\n",
        "\n",
        "print(\"     Original Image Resolution\")\n",
        "print('----------------------------------')\n",
        "display_single_image(img_path, img_size, print_shape = True)\n",
        "\n",
        "print(\"  Resolution After Preprocessing\")\n",
        "print('----------------------------------')\n",
        "display_single_image(img_path, img_size2, print_shape = True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDM2e4iv1_Au"
      },
      "source": [
        "Basic CNN Model (64x64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2BI6Vrb1-os"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(64, (3,3), activation='relu', name = 'last_conv_layer'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=[\n",
        "                  'accuracy',\n",
        "                  Precision(name='precision'),\n",
        "                  Recall(name='recall'),\n",
        "                  AUC(name='auc')\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "basic_history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=10,\n",
        "    validation_data=val_ds\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Kl0yudb3WPN"
      },
      "source": [
        "Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNITwz533Ub4"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/TCC-Dataset/Models/basic_model.keras')\n",
        "\n",
        "with open('/content/drive/MyDrive/TCC-Dataset/History/basic_history.pkl', 'wb') as f:\n",
        "    pickle.dump(basic_history.history, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo2Wi_-h3lPM"
      },
      "source": [
        "Enhanced CNN Model (64x64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Gd8hHjm3uOp"
      },
      "outputs": [],
      "source": [
        "callback = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=4, mode='max',\n",
        "    min_delta=0.0,\n",
        "    restore_best_weights=True)\n",
        "\n",
        "new_model = Sequential()\n",
        "\n",
        "new_model.add(Conv2D(128, (3,3), activation='relu', input_shape=(64, 64 , 3)))\n",
        "new_model.add(BatchNormalization(momentum=0.95, epsilon=0.005))\n",
        "new_model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "new_model.add(Conv2D(64, (3,3), activation='tanh'))\n",
        "new_model.add(BatchNormalization(momentum=0.95, epsilon=0.005))\n",
        "new_model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "new_model.add(Conv2D(32, (3,3), activation='tanh'))\n",
        "new_model.add(BatchNormalization(momentum=0.95, epsilon=0.005))\n",
        "new_model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "new_model.add(Conv2D(16, (3,3), activation='tanh'))\n",
        "new_model.add(BatchNormalization(momentum=0.95, epsilon=0.005))\n",
        "new_model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "new_model.add(Flatten())\n",
        "new_model.add(Dense(64, activation='relu'))\n",
        "new_model.add(Dense(32, activation='relu'))\n",
        "new_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "new_model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        Precision(name='precision'),\n",
        "        Recall(name='recall'),\n",
        "        AUC(name='auc')\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(new_model.summary())\n",
        "\n",
        "enhanced_history= new_model.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs=15,\n",
        "    batch_size = 32,\n",
        "    callbacks = callback,\n",
        ")\n",
        "\n",
        "new_model.save('/content/drive/MyDrive/TCC-Dataset/Models/enhanced_model.keras')\n",
        "\n",
        "with open('/content/drive/MyDrive/TCC-Dataset/History/enhanced_history.pkl', 'wb') as f:\n",
        "    pickle.dump(enhanced_history.history, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKBTV86M3zYu"
      },
      "source": [
        "Saving the model / Saving training history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zd3Q3qY3wjl"
      },
      "outputs": [],
      "source": [
        "new_model.save('/content/drive/MyDrive/TCC-Dataset/Models/enhanced_model(backup).keras')\n",
        "\n",
        "with open('/content/drive/MyDrive/TCC-Dataset/History/enhanced_history(backup).pkl', 'wb') as f:\n",
        "    pickle.dump(enhanced_history.history, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCqtDamW4BD_"
      },
      "source": [
        "Xception Model (150x150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwIKzRvS4ARP"
      },
      "outputs": [],
      "source": [
        "callback = EarlyStopping(\n",
        "    monitor='val_acc',\n",
        "    patience=3,\n",
        "    mode='max',\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "model_xception = Sequential()\n",
        "\n",
        "conv_base = Xception(weights = 'imagenet', include_top = False, input_shape=(150, 150, 3),\n",
        "                     classifier_activation = 'softmax')\n",
        "conv_base.trainable = False\n",
        "\n",
        "model_xception.add(conv_base)\n",
        "\n",
        "model_xception.add(Flatten())\n",
        "\n",
        "model_xception.add(Dense(16, activation = 'relu'))\n",
        "model_xception.add(Dense(32, activation = 'relu'))\n",
        "model_xception.add(Dense(64, activation = 'relu'))\n",
        "model_xception.add(Dense(128, activation = 'relu'))\n",
        "model_xception.add(Dense(256, activation = 'relu'))\n",
        "\n",
        "model_xception.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "model_xception.compile(optimizer = 'adam',\n",
        "                       loss = 'binary_crossentropy',\n",
        "                       metrics = ['acc'])\n",
        "\n",
        "print(model_xception.summary())\n",
        "\n",
        "xception_history = model_xception.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = 20,\n",
        "    callbacks = [callback]\n",
        ")\n",
        "\n",
        "model_xception.save('/content/drive/MyDrive/TCC-Dataset/Models/xception_model.keras')\n",
        "\n",
        "with open('/content/drive/MyDrive/TCC-Dataset/History/xception_history.pkl', 'wb') as f:\n",
        "    pickle.dump(xception_history.history, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Existing Xception Model for comparison (299x299)\n",
        "\n",
        "*Needed some changes to work on our dataset"
      ],
      "metadata": {
        "id": "BfcHDR1EkWGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Early Stopping\n",
        "callback = EarlyStopping(monitor = 'val_acc', patience = 3, mode = 'max',\n",
        "                         min_delta = .01, restore_best_weights = True, verbose = 2)\n",
        "\n",
        "# Create model\n",
        "model_3 = Sequential()\n",
        "\n",
        "# Create the base with Xception\n",
        "conv_base = Xception(weights = 'imagenet', include_top = True,\n",
        "                     classifier_activation = 'softmax')\n",
        "conv_base.trainable = False\n",
        "model_3.add(conv_base)\n",
        "\n",
        "model_3.add(Flatten())\n",
        "\n",
        "# Add Dense Layers\n",
        "model_3.add(Dense(256, activation = 'tanh'))\n",
        "model_3.add(Dense(128, activation = 'tanh'))\n",
        "model_3.add(Dense(64, activation = 'tanh'))\n",
        "model_3.add(Dense(32, activation = 'tanh'))\n",
        "model_3.add(Dense(16, activation = 'tanh'))\n",
        "\n",
        "# Output layer\n",
        "model_3.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "# Compile\n",
        "model_3.compile(optimizer = 'adam', loss = 'binary_crossentropy',\n",
        "              metrics = ['acc'])\n",
        "\n",
        "# Get the model summary\n",
        "print(model_3.summary())\n",
        "\n",
        "# Fit Model\n",
        "history_3 = model_3.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = 25,\n",
        "    callbacks = [callback]\n",
        ")"
      ],
      "metadata": {
        "id": "4LSMHV5PkVg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ViT Model (224x224)"
      ],
      "metadata": {
        "id": "e0igSbivX22B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_DIR = '/content/drive/MyDrive/TCC-Dataset/ModelTraining'\n",
        "\n",
        "# Hiperparâmetros\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 1\n",
        "patch_size = 16\n",
        "num_patches = (input_shape[0] // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "transformer_units = [projection_dim * 2, projection_dim]  # FFN\n",
        "transformer_layers = 8\n",
        "num_heads = 4\n",
        "mlp_head_units = [256, 128]  # cabeça final\n",
        "\n",
        "# Camada para dividir a imagem em patches\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "# Camada de embedding dos patches\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "inputs = layers.Input(shape=input_shape)\n",
        "augmented = layers.Rescaling(1./255)(inputs)\n",
        "\n",
        "patches = Patches(patch_size)(augmented)\n",
        "encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "# Transformer blocks\n",
        "for _ in range(transformer_layers):\n",
        "    # Layer normalization 1\n",
        "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    # Multi-head Self Attention\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "    )(x1, x1)\n",
        "    # Skip connection 1\n",
        "    x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "    # Layer normalization 2\n",
        "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "    x3 = layers.Dense(transformer_units[0], activation=tf.nn.gelu)(x3)\n",
        "    x3 = layers.Dropout(0.1)(x3)\n",
        "    x3 = layers.Dense(transformer_units[1], activation=tf.nn.gelu)(x3)\n",
        "    x3 = layers.Dropout(0.1)(x3)\n",
        "    encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "# Classificação\n",
        "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "representation = layers.Flatten()(representation)\n",
        "for units in mlp_head_units:\n",
        "    representation = layers.Dense(units, activation=\"relu\")(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(representation)\n",
        "\n",
        "vit_model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "vit_model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-3),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.AUC(name='auc'), 'accuracy']\n",
        ")\n",
        "\n",
        "vit_model.summary()\n",
        "vit_model.save(os.path.join(SAVE_DIR, 'ViT_Model.keras'))\n"
      ],
      "metadata": {
        "id": "x9DlTwi9XHE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hybrid model (ViT + Xception) (224x224)"
      ],
      "metadata": {
        "id": "XqiamuxTYeGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import xception\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_auc', patience=5, mode='max', verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3),\n",
        "    ModelCheckpoint('hybrid_model.keras', save_best_only=True),\n",
        "    TerminateOnNaN()\n",
        "]\n",
        "\n",
        "SAVE_DIR = '/content/drive/MyDrive/TCC-Dataset/ModelTraining'\n",
        "\n",
        "INPUT_SHAPE = (224, 224, 3)\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "PATCH_SIZE = 16\n",
        "PROJECTION_DIM = 256\n",
        "NUM_HEADS = 8\n",
        "TRANSFORMER_LAYERS = 8\n",
        "FFN_UNITS = [\n",
        "    PROJECTION_DIM * 4,\n",
        "    PROJECTION_DIM,\n",
        "]\n",
        "DROPOUT_RATE = 0.1\n",
        "\n",
        "#Fine-tunning\n",
        "FINE_TUNE_AT = -20\n",
        "\n",
        "# --- Componentes do ViT (reutilizando do código anterior) ---\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "# --- Create hybrid model ---\n",
        "\n",
        "def create_hybrid_model():\n",
        "\n",
        "    # Uma única entrada que será usada por ambos os ramos.\n",
        "    inputs = layers.Input(shape=INPUT_SHAPE, name=\"input_image\")\n",
        "\n",
        "    # Pré-processamento específico para a Xception\n",
        "    xception_preprocessed = xception.preprocess_input(inputs)\n",
        "\n",
        "    # Carrega a base da Xception\n",
        "    xception_base = xception.Xception(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=INPUT_SHAPE,\n",
        "        pooling=None\n",
        "    )\n",
        "\n",
        "    # Congela a base para não destruir os pesos durante o treino inicial\n",
        "    xception_base.trainable = False\n",
        "\n",
        "    cnn_features = xception_base(xception_preprocessed, training=False)\n",
        "    cnn_features = layers.GlobalAveragePooling2D(name=\"cnn_global_pooling\")(cnn_features)\n",
        "\n",
        "\n",
        "    #  ViT\n",
        "\n",
        "    # Calcula o número de patches\n",
        "    num_patches = (INPUT_SHAPE[0] // PATCH_SIZE) ** 2\n",
        "\n",
        "    # Normaliza os pixels para o ViT (0-1)\n",
        "    vit_rescaled = layers.Rescaling(1./255)(inputs)\n",
        "\n",
        "    # Criação dos Patches\n",
        "    patches = Patches(PATCH_SIZE, name=\"vit_patches\")(vit_rescaled)\n",
        "\n",
        "    # Codificação dos Patches (Projeção Linear + Embedding de Posição)\n",
        "    encoded_patches = PatchEncoder(num_patches, PROJECTION_DIM, name=\"vit_patch_encoder\")(patches)\n",
        "\n",
        "    # Blocos Transformer\n",
        "    for i in range(TRANSFORMER_LAYERS):\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=NUM_HEADS, key_dim=PROJECTION_DIM // NUM_HEADS, dropout=DROPOUT_RATE\n",
        "        )(x1, x1)\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x4 = layers.Dense(FFN_UNITS[0], activation=tf.nn.gelu)(x3)\n",
        "        x4 = layers.Dropout(DROPOUT_RATE)(x4)\n",
        "        x4 = layers.Dense(FFN_UNITS[1])(x4)\n",
        "        x4 = layers.Dropout(DROPOUT_RATE)(x4)\n",
        "        encoded_patches = layers.Add()([x4, x2])\n",
        "\n",
        "    # Representação final do ViT (pooling)\n",
        "    vit_representation = layers.LayerNormalization(epsilon=1e-6, name=\"vit_layer_norm\")(encoded_patches)\n",
        "    vit_features = layers.GlobalAveragePooling1D(name=\"vit_global_pooling\")(vit_representation)\n",
        "\n",
        "\n",
        "    # --- Model Fusion ---\n",
        "\n",
        "    # Concatena os vetores de características da CNN e do ViT\n",
        "    merged_features = layers.Concatenate(name=\"feature_fusion\")([cnn_features, vit_features])\n",
        "\n",
        "    x = layers.Dense(512, activation=\"relu\")(merged_features)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    outputs = layers.Dense(NUM_CLASSES, activation=\"sigmoid\", name=\"output_classifier\")(x)\n",
        "\n",
        "    # Cria o modelo final\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"Hybrid_Xception_ViT\")\n",
        "\n",
        "    for layer in model.get_layer('xception').layers[FINE_TUNE_AT:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    return model\n",
        "\n",
        "hybrid_model = create_hybrid_model()\n",
        "\n",
        "optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=1e-5)\n",
        "hybrid_model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.AUC(name='auc'), 'accuracy']\n",
        ")\n",
        "\n",
        "hybrid_model.summary()\n",
        "\n",
        "history = hybrid_model.fit(\n",
        "    train_ds,\n",
        "    epochs=50,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "\n",
        "hybrid_model.save(os.path.join(SAVE_DIR, 'hibrid_model_final.keras'))"
      ],
      "metadata": {
        "id": "E6t9qJEVYgMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzDpcSjc4eGY"
      },
      "source": [
        "Plotting single image to see if the images on the dataset are correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbfbzFkM4dv_"
      },
      "outputs": [],
      "source": [
        "for images, labels in val_ds.take(1):\n",
        "    plt.imshow(images[0].numpy())\n",
        "    plt.title(\"IA\" if labels[0].numpy() == 1 else \"Humano\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MOeGgW6oNL-"
      },
      "source": [
        "Testing the performance of the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jyfMU4yq4v-"
      },
      "outputs": [],
      "source": [
        "model_evaluation(model, test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWYwh64m4mEQ"
      },
      "source": [
        "Plotting the loss/acc graphs of a model after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ESoogS54lru"
      },
      "outputs": [],
      "source": [
        "# Gráfico de Loss\n",
        "plt.plot(enhanced_history.history['loss'], label='Train Loss')\n",
        "plt.plot(enhanced_history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Gráfico de Acurácia\n",
        "plt.plot(enhanced_history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(enhanced_history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Acurácia')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg7NW0s45Gpn"
      },
      "source": [
        "Loading a model from the drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8Wxjod55GX9"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/MyDrive/TCC-Dataset/Models/enhanced_model(93%).keras'\n",
        "enhanced_model = load_model(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-2lJFgf5Ow8"
      },
      "source": [
        "Gradcam explanation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmNrtDkO5OOM"
      },
      "outputs": [],
      "source": [
        "def make_gradcam_heatmap(img_array, grad_model):\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        class_idx = tf.argmax(predictions[0]) if predictions.shape[1] > 1 else 0\n",
        "        loss = predictions[:, class_idx]\n",
        "\n",
        "    grads = tape.gradient(loss, conv_outputs)\n",
        "\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "def superimpose_heatmap(img_path, heatmap, alpha=0.4):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, (150, 150))\n",
        "\n",
        "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "    heatmap_colormap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "    superimposed_img = heatmap_colormap * alpha + img * (1 - alpha)\n",
        "    superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return superimposed_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2GwhuFr5Wu1"
      },
      "source": [
        "Visualizing the heatmap using gradcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6Byv8WOdFNP"
      },
      "outputs": [],
      "source": [
        "# Pré-processamento (igual ao treino)\n",
        "def rescale_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, (64, 64))\n",
        "    img = img / 255.0\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6Xy1v7x5WZN"
      },
      "outputs": [],
      "source": [
        "#model = load_model('/content/drive/MyDrive/TCC-Dataset/modelo_treinado.h5')\n",
        "img_path = '/content/drive/MyDrive/TCC-Dataset/backup/test/fake/0_01_27__hugging_happy__ZYCZ30C0_61.png'\n",
        "grad_model = Model(inputs=enhanced_model.inputs, outputs=[enhanced_model.get_layer('conv2d_3').output, enhanced_model.output])\n",
        "\n",
        "# Pré-processa a imagem\n",
        "img_array = prepare_image(img_path)\n",
        "\n",
        "# Predição + Heatmap\n",
        "conv_outputs, pred = grad_model.predict(img_array)\n",
        "heatmap = make_gradcam_heatmap(img_array, grad_model)\n",
        "\n",
        "# Visualização\n",
        "superimposed_img = superimpose_heatmap(img_path, heatmap)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(plt.imread(img_path))\n",
        "plt.title('Original')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(superimposed_img)\n",
        "plt.colorbar(label='Intensidade do Heatmap')\n",
        "plt.title(f'Imagem Real? {\"Sim\" if pred[0][0] > 0.5 else \"Não\"} (Prob: {pred[0][0]:.2f})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdjiC04X5j5Q"
      },
      "source": [
        "Feature maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Kv6dnps5mER"
      },
      "outputs": [],
      "source": [
        "layer_outputs = [layer.output for layer in new_model.layers if 'conv' in layer.name]\n",
        "activation_model = Model(inputs=new_model.input, outputs=layer_outputs)\n",
        "\n",
        "img = next(iter(val_ds))[0][0].numpy()\n",
        "img_batch = np.expand_dims(img, axis=0)\n",
        "\n",
        "activations = activation_model.predict(img_batch)\n",
        "\n",
        "first_layer_activation = activations[0]\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i in range(6):\n",
        "    plt.subplot(1, 6, i+1)\n",
        "    plt.imshow(first_layer_activation[0, :, :, i], cmap='viridis')\n",
        "    plt.axis('off')\n",
        "plt.suptitle(\"Ativações da primeira camada convolucional\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU-mRtWd5roJ"
      },
      "source": [
        "Lime explainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skHq2OYk5stO"
      },
      "outputs": [],
      "source": [
        "# Pré-processamento da imagem\n",
        "def prepare_image(file_path):\n",
        "    img = tf.keras.utils.load_img(file_path, target_size=(64, 64))\n",
        "    img = tf.keras.utils.img_to_array(img) / 255.0  # Normaliza para [0, 1]\n",
        "    return img\n",
        "\n",
        "def predict_fn(images):\n",
        "    images = np.array(images)\n",
        "    return enhanced_model.predict(images)\n",
        "\n",
        "img = prepare_image('/content/drive/MyDrive/TCC-Dataset/backup/test/fake/0_01_27__hugging_happy__ZYCZ30C0_61.png')\n",
        "\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "explanation = explainer.explain_instance(\n",
        "    img,\n",
        "    classifier_fn=predict_fn,\n",
        "    top_labels=1,\n",
        "    hide_color=0,\n",
        "    num_samples=1000\n",
        ")\n",
        "\n",
        "# Visualização\n",
        "temp, mask = explanation.get_image_and_mask(\n",
        "    explanation.top_labels[0],\n",
        "    positive_only=True,\n",
        "    hide_rest=False,\n",
        "    num_features=6,\n",
        "    min_weight=0.01\n",
        ")\n",
        "\n",
        "plt.imshow(mark_boundaries(temp, mask))\n",
        "plt.title(\"Explicação com LIME para classe 'Imagem de IA'\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
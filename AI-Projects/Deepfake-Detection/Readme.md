# ğŸ§  Deepfake Detection: A Comparative Study of CNNs, Transfer Learning, and Vision Transformers

This project is part of my Computer Science graduation thesis and focuses on analyzing and comparing different neural network architectures for the task of **deepfake detection**.  
It was developed entirely in a **Jupyter Notebook (Google Colab)** environment and extensively documented, explaining the reasoning and results of each experiment.

---

## ğŸ“˜ Overview

The main goal of this project is to study how different model architectures perform when distinguishing **AI-generated (fake)** images from **real** ones.  
The experiments include:

- **Baseline CNN models** â€” Simple convolutional architectures built from scratch.
- **Transfer Learning with Xception** â€” Using a pre-trained Xception network to detect local facial artifacts.
- **Vision Transformer (ViT)** â€” Exploring transformer-based architectures for global artifact detection.
- **Hybrid Model (ViT + Xception)** â€” A custom model combining the strengths of ViTs (global artifacts) and CNNs (local artifacts).

Several variations and optimizations were tested during development, but only the **main representative models** are included for clarity and reproducibility.

---

## âš™ï¸ How to Run

The notebook requires access to a **Google Drive** containing a **deepfake dataset**.  
Due to security and privacy reasons, the dataset used in this study **will not be publicly shared**.

To run the project:

1. Open the notebook in Google Colab.  
2. Mount your Drive containing a deepfake dataset.  
3. Follow the notebook cells in order â€” each section is clearly explained.

---

## ğŸ§© Project Structure

DeepfakeDetection/

â”‚

â”œâ”€â”€ DeepfakeDetection.ipynb # Main Jupyter Notebook

â”œâ”€â”€ README.md # Project documentation

â””â”€â”€ requirements.txt # Dependencies


---

## ğŸ§  Technologies Used

- Python 3.x  
- TensorFlow / Keras  
- NumPy / Pandas / Matplotlib  
- OpenCV   
- Vision Transformers (ViT)  

---

## ğŸ“ˆ Experimental Setup

Each architecture was trained and evaluated using the same dataset split to ensure fair comparison.  
Metrics such as **accuracy**, **F1-score**, **AUC** and **confusion matrices** were used to assess model performance.

---

## ğŸ”¬ Results Summary

| Model Type | Description | Main Strength | Performance |
|-------------|--------------|----------------|--------------|
| **Baseline CNN** | Simple custom-built CNN | Fast, low complexity | Moderate |
| **Xception (Transfer Learning)** | Pretrained CNN for local features | Excellent for fine-grained facial cues | High |
| **Vision Transformer (ViT)** | Transformer-based global feature extractor | Strong at detecting global inconsistencies | High |
| **Hybrid (ViT + Xception)** | Combines CNN and ViT components | Captures both local and global artifacts | **Best overall** |

---

## âš ï¸ Dataset Notice

The dataset used in this research contains real and AI-generated facial images.  
Due to ethical and security reasons, it will **not be shared publicly**.  
However, the notebook is compatible with most public deepfake datasets (e.g., FaceForensics++, DeepFake Detection Challenge dataset, etc.) â€” paths may need minor adjustments.

---

## ğŸ§¾ License

This repository is intended for **academic and educational purposes**.  
If you wish to use or reference this work, please include proper attribution.

---

## ğŸ‘¤ Author

**Leonardo Melo**  
Bachelorâ€™s Degree in Computer Science  
[GitHub](https://github.com/LeoMeloL) 

---
